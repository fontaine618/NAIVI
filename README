# Node attribute imputation using variational inference (NAIVI)

Author: **Simon Fontaine** (simfont@umich.edu)

## To do list

### VMP 

[x] Implement affine parameters update
[x] Implement ELBO for logistic fragment
    [x] Quadratic
    [x] MK
    [x] Tilted
[ ] Remove grad computation for paramters
[ ] Check M Step with new VMP
[x] Implement wrapper similar to ADVI/VIMC
[x] ELBO: MC at each factor (to check elbo computation/Logistic LB)
[x] ELBO: Like VIMC (to check messages)
[x] Figure out why mean is never updated ???
    [x] Break symmetry?
[x] Profile and make faster
    - more than 50% time is spent inverting matrices
    - need to check if some can be avoided, by I think this
    is the best I can do ...
[x] Add true values with some metrics
    [x] Do a fit_and_evaluate method
[ ] Initialization
    [ ] Use ADVI first? VMP is then just a refinement?
    [x] I think just breaking symmetry should be fine ...
[x] Memoize covariances:
    - if instantiated from mean and covariance, store them too
    - when variance is called, check if it is stored. If yes, just return.
      If no, compute it and store it.
[ ] JIT?
[ ] ELBO MC probably better for early stopping & model selection:
    - Indeed, I note that the MC one seems to increase with iteration
      (up to noise), while the LB one decreaases at some point.
[ ] MNAR model
    - should be fairly easy
    - add an argument mnar: bool
    - triggers _initialize_missingness_model
    - which reads in the two covariate models to see where the missing values are
    - create a binary model just like the binary covariates
    - parameter update will need a proximal step
[ ] Multinomial model

remember to run with -O

## Changelog

### Version 0.1.0

Implemented methods:
- Stochastic VI with reparameterization trick (VIMC)
- Direct optimization (ADVI)
- MLE & MAP
- MICE, MissForest, mean imputation (no network)
- MCMC (No missing values allowed)
- NerworkSmoothing: average of the neighbors until stable

Under development:
- Variational message passing (VMP) with multivariate normal variational approximation

MEAN
tensor([[ 0.3287, -1.3679],
        [-0.3921,  1.1980],
        [-0.0114, -0.1906],
        [ 0.1439, -0.7756],
        [-0.1230,  0.1026],
        [-0.0114, -0.1906],
        [-0.3215,  0.9920],
        [ 0.0368, -0.3811],
        [ 0.0895, -0.5071],
        [ 0.2528, -1.1681]])

 VARIANCE
 tensor([[0.0117, 0.1238],
        [0.0092, 0.0851],
        [0.0342, 0.4031],
        [0.0151, 0.1510],
        [0.0163, 0.2331],
        [0.0342, 0.4031],
        [0.0196, 0.1659],
        [0.0344, 0.4058],
        [0.0183, 0.1556],
        [0.0164, 0.1626]])

 OBSERVATIONS
 tensor([[0.5814, 0.0000],
        [0.0000, 1.0000],
        [0.4971, 0.4522],
        [0.5359, 0.3151],
        [1.0000, 0.5257],
        [0.4971, 0.4522],
        [0.4203, 1.0000],
        [0.0000, 0.0000],
        [1.0000, 0.0000],
        [0.5629, 0.2371]])

QUADRATIC
tensor([[ 0.0000, -0.2402],
        [-0.5173, -0.2732],
        [ 0.0000,  0.0000],
        [ 0.0000,  0.0000],
        [-0.7586,  0.0000],
        [ 0.0000,  0.0000],
        [ 0.0000, -0.3345],
        [-0.7160, -0.5699],
        [-0.6517, -0.4903],
        [ 0.0000,  0.0000]])

QUADRATURE
tensor([[ 0.0000,  1.1310],
        [-0.1251, -0.2713],
        [ 0.0000,  0.0000],
        [ 0.0000,  0.0000],
        [-0.7586,  0.0000],
        [ 0.0000,  0.0000],
        [ 0.0000, -0.3317],
        [-0.7527, -0.1865],
        [-0.6517,  0.0177],
        [ 0.0000,  0.0000]])

TILTED
tensor([[ 0.0000,  1.1309],
        [-0.1251, -0.2714],
        [ 0.0000,  0.0000],
        [ 0.0000,  0.0000],
        [-0.7586,  0.0000],
        [ 0.0000,  0.0000],
        [ 0.0000, -0.3319],
        [-0.7528, -0.1887],
        [-0.6517,  0.0174],
        [ 0.0000,  0.0000]])

MC
tensor([[    nan, -0.2399],
        [-0.5173, -0.2707],
        [    nan,     nan],
        [    nan,     nan],
        [-0.7596,     nan],
        [    nan,     nan],
        [    nan, -0.3303],
        [-0.7104, -0.5691],
        [-0.6555, -0.4924],
        [    nan,     nan]])